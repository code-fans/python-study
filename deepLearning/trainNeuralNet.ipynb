{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import sigmoid, softmax, sigmoid_grad\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, layerSize, weight_init_std=0.01, h = sigmoid, sigma = softmax):\n",
    "        # 初始化权重\n",
    "        self.l = len(layerSize) - 1\n",
    "        self.h = h\n",
    "        self.sigma = sigma\n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        for i in range(self.l):\n",
    "            self.W.append(weight_init_std * np.random.randn( layerSize[i], layerSize[i+1]))\n",
    "            self.B.append(np.zeros(layerSize[i+1]))\n",
    "\n",
    "    def calcLayer(A, w, b, h):\n",
    "        return h(np.dot(A,w) + b)\n",
    "                          \n",
    "    def predict(self, x):\n",
    "        A = x\n",
    "        # 隐藏层\n",
    "        for i in range(self.l - 1):\n",
    "            A = calcLayer(A, self.W[i], self.B[i], self.h)\n",
    "        # 输出层\n",
    "        return calcLayer(A, W[self.l-1], B[self.l-1], self.sigma)\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = []\n",
    "        for i in range(self.l - 1):\n",
    "            grads.append(numerical_gradient(loss_W, self.W[i]))\n",
    "            grads.append(numerical_gradient(loss_W, self.B[i]))\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):        \n",
    "        batch_num = x.shape[0]\n",
    "        # forward\n",
    "        # 输入层\n",
    "        A = []\n",
    "        Z = []\n",
    "        z = x\n",
    "        A.append(x)\n",
    "        # 隐藏层\n",
    "        for i in range(self.l - 1):\n",
    "            a = np.dot(z, self.W[i]) + self.B[i]\n",
    "            z = self.h(a)\n",
    "            A.append(a)\n",
    "            Z.append(z)\n",
    "        # 输出层\n",
    "        a = np.dot(z, self.W[self.l-1]) + self.B[self.l-1]\n",
    "        y = self.sigma(a)\n",
    "        \n",
    "        A.append(a)\n",
    "        Z.append(y)\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        \n",
    "        grads = []\n",
    "        for i in range(self.l-1,-1,-1):\n",
    "            grads.append(np.dot( A[i].T, dy))\n",
    "            grads.append(np.sum(dy, axis=0))\n",
    "            \n",
    "            da = np.dot(dy, self.W[i].T)\n",
    "            dy = sigmoid_grad(A[i]) * da\n",
    "        # numerical_gradient 和这个返回的顺序一致\n",
    "        return grads[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
